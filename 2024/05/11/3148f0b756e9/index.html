<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>pytorch导学：使用bert实现分类任务 | 爱开源GoGo</title><meta name="author" content="JimmyDing"><meta name="copyright" content="JimmyDing"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="bert模型使用 huggingface中的模型 huggingface中的模型定义的基本结构如下： 123456789101112131415161718192021222324def _forward_unimplemented(self, *input: Any) -&gt; None:    raise NotImplementedError(f&quot;Module [&#123;ty">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch导学：使用bert实现分类任务">
<meta property="og:url" content="https://mylofty.github.io/2024/05/11/3148f0b756e9/index.html">
<meta property="og:site_name" content="爱开源GoGo">
<meta property="og:description" content="bert模型使用 huggingface中的模型 huggingface中的模型定义的基本结构如下： 123456789101112131415161718192021222324def _forward_unimplemented(self, *input: Any) -&gt; None:    raise NotImplementedError(f&quot;Module [&#123;ty">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2024-05-11T06:51:21.000Z">
<meta property="article:modified_time" content="2025-05-09T07:04:26.765Z">
<meta property="article:author" content="JimmyDing">
<meta property="article:tag" content="ai">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/images/notebook.png"><link rel="canonical" href="https://mylofty.github.io/2024/05/11/3148f0b756e9/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch导学：使用bert实现分类任务',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-09 15:04:26'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/notebook.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">32</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 图库</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="爱开源GoGo"><span class="site-name">爱开源GoGo</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 图库</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch导学：使用bert实现分类任务</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-11T06:51:21.000Z" title="发表于 2024-05-11 14:51:21">2024-05-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-09T07:04:26.765Z" title="更新于 2025-05-09 15:04:26">2025-05-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/">ai</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="pytorch导学：使用bert实现分类任务"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="bert模型使用">bert模型使用</h2>
<h3 id="huggingface中的模型">huggingface中的模型</h3>
<p>huggingface中的模型定义的基本结构如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_forward_unimplemented</span>(<span class="params">self, *<span class="built_in">input</span>: <span class="type">Any</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">f&quot;Module [<span class="subst">&#123;<span class="built_in">type</span>(self).__name__&#125;</span>] is missing the required \&quot;forward\&quot; function&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.basename = <span class="string">&quot;BaseModel&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, *args: <span class="type">Any</span>, **kwds: <span class="type">Any</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;now you call __call__,params=<span class="subst">&#123;args&#125;</span>&quot;</span>)</span><br><span class="line">        self.forward(args[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    forward: <span class="type">Callable</span>[..., <span class="type">Any</span>] = _forward_unimplemented</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        self.name = name</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;now you call forward,param=<span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = MyModel(<span class="string">&quot;bert&quot;</span>)</span><br><span class="line">model([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment"># now you call __call__,params=([1, 2, 3, 4],)</span></span><br><span class="line"><span class="comment"># now you call forward,param=[1, 2, 3, 4]</span></span><br></pre></td></tr></table></figure>
<h3 id="加载bert模型">加载bert模型</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Callable</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;google-bert/bert-base-uncased&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;tokenizer type=&#x27;</span>,<span class="built_in">type</span>(tokenizer))</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;google-bert/bert-base-uncased&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载bert模型推理</span></span><br><span class="line">inputs = tokenizer(<span class="string">&quot;你是个好人&quot;</span>, return_tensors=<span class="string">&#x27;pt&#x27;</span>) <span class="comment"># 返回数据类型可以为pt，也可以为tf</span></span><br><span class="line">output = model(**inputs) <span class="comment"># 打印tensor</span></span><br></pre></td></tr></table></figure>
<h3 id="bert中分词tokenize">bert中分词tokenize</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;google-bert/bert-base-uncased&quot;</span>)  </span><br><span class="line"><span class="comment"># huggingface默认下载位置：~\.cache\huggingface\hub，可以配置环境变量export HF_HOME=&quot;目标地址&quot;修改</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;词典大小:&quot;</span>,tokenizer.vocab_size)</span><br><span class="line"><span class="comment"># 词典大小: 30522 对应vocab.txt行数</span></span><br><span class="line">text = <span class="string">&quot;the game has gone!unaffable  I have a new GPU!&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;英文分词, 英文：<span class="subst">&#123;text&#125;</span>，分词：<span class="subst">&#123;tokens&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 英文分词, 英文：the game has gone!unaffable  I have a new GPU!，分词：[&#x27;the&#x27;, &#x27;game&#x27;, &#x27;has&#x27;, &#x27;gone&#x27;, &#x27;!&#x27;, &#x27;una&#x27;, &#x27;##ffa&#x27;, &#x27;##ble&#x27;, &#x27;i&#x27;, &#x27;have&#x27;, &#x27;a&#x27;, &#x27;new&#x27;, &#x27;gp&#x27;, &#x27;##u&#x27;, &#x27;!&#x27;]</span></span><br><span class="line">text = <span class="string">&quot;我爱北京天安门，吢吣&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;中文分词, 中文：<span class="subst">&#123;text&#125;</span>，分词：<span class="subst">&#123;tokens&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 中文分词, 英文：我爱北京天安门，吢吣，分词：[&#x27;我&#x27;, &#x27;[UNK]&#x27;, &#x27;北&#x27;, &#x27;京&#x27;, &#x27;天&#x27;, &#x27;安&#x27;, &#x27;[UNK]&#x27;, &#x27;，&#x27;, &#x27;[UNK]&#x27;, &#x27;[UNK]&#x27;]</span></span><br><span class="line">input_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;id-token转换:&quot;</span>,input_ids)</span><br><span class="line"><span class="comment"># id-token转换: [1855, 100, 1781, 1755, 1811, 1820, 100, 1989, 100, 100]</span></span><br><span class="line">sen_code = tokenizer.encode_plus(<span class="string">&quot;i like  you  much&quot;</span>, <span class="string">&quot;but not him&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;多句子encode：&quot;</span>,sen_code)</span><br><span class="line"><span class="comment"># 句子encode： &#123;&#x27;input_ids&#x27;: [101, 1045, 2066, 2017, 2172, 102, 2021, 2025, 2032, 102], &#x27;token_type_ids&#x27;: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1], &#x27;attention_mask&#x27;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;decode：&quot;</span>,tokenizer.decode(sen_code[<span class="string">&#x27;input_ids&#x27;</span>]))</span><br><span class="line"><span class="comment"># decode： [CLS] i like you much [SEP] but not him [SEP]</span></span><br><span class="line">inputs = tokenizer(<span class="string">&quot;你好&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;tokenizer(&quot;你好&quot;)=<span class="subst">&#123;inputs&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># tokenizer(&quot;你好&quot;)=&#123;&#x27;input_ids&#x27;: tensor([[101, 100, 100, 102]]), &#x27;token_type_ids&#x27;: tensor([[0, 0, 0, 0]]), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1]])&#125;</span></span><br><span class="line">inputs = tokenizer([<span class="string">&quot;你好吗&quot;</span>,<span class="string">&quot;。不好的。哈哈&quot;</span>], padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="comment"># 多个句子，按最长的进行填充。同时设置最长不超过128</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;tokenizer(&quot;你好吗。不好的。哈哈&quot;)=<span class="subst">&#123;inputs&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># tokenizer(&quot;你好吗。不好的。哈哈&quot;)=&#123;&#x27;input_ids&#x27;: tensor([[ 101,  100,  100,  100,  102,    0,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#         [ 101, 1636, 1744,  100, 1916, 1636,  100,  100,  102]]), &#x27;token_type_ids&#x27;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 0, 0, 0, 0]]), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1, 1, 1, 1, 1, 1]])&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Bert的tokenizer中有特殊标记（Special Tokens）。它们的含义如下：</p>
<ul>
<li>[PAD]：在batch中对齐序列长度时，用 [PAD]进行填充以使所有序列长度相同。可以通过将其添加到较短的序列末尾来实现对齐。</li>
<li>[CLS]：在输入序列的开头添加 [CLS] 标记，以表示该序列的分类结果。</li>
<li>[SEP]：用于分隔两个句子，例如在文本分类问题中，将两个句子拼接成一个输入序列时，可以使用 [SEP] 来分隔这两个句子。</li>
<li>[UNK]：此标记用于表示未知或词汇外的单词。当一个模型遇到一个它以前没有见过/无法识别的词时，它会用这个标记替换它。</li>
</ul>
<h3 id="AutoTokenizer到bert-tokenizer">AutoTokenizer到bert tokenizer</h3>
<p>对于bert模型，模型文件格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># .cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/</span></span><br><span class="line">.</span><br><span class="line">├── config.json -&gt; ../../blobs/45a2321a7ecfdaaf60a6c1fd7f5463994cc8907d</span><br><span class="line">├── model.safetensors -&gt; ../../blobs/68d45e234eb4a928074dfd868cead0219ab85354cc53d20e772753c6bb9169d3</span><br><span class="line">├── tokenizer_config.json -&gt; ../../blobs/e5c73d8a50df1f56fb5b0b8002d7cf4010afdccb</span><br><span class="line">├── tokenizer.json -&gt; ../../blobs/949a6f013d67eb8a5b4b5b46026217b888021b88</span><br><span class="line">└── vocab.txt -&gt; ../../blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938</span><br></pre></td></tr></table></figure>
<p>调用<code>tokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-uncased&quot;) </code>后，<code>from_pretrained</code>函数首先调用<code>get_tokenizer_config</code>读取<code>tokenizer_config.json</code>文件中的数据，</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tokenizer_config.json</span></span><br><span class="line">&#123;<span class="string">&quot;do_lower_case&quot;</span>: true, <span class="string">&quot;model_max_length&quot;</span>: <span class="number">512</span>&#125;</span><br><span class="line"><span class="comment"># 很多自有模型都会提供如tokenizer_class=&quot;QWenTokenizer&quot;, &quot;auto_map&quot;:&#123;&quot;AutoTokenizer&quot;:&quot;tokenization_qwen.QWenTokenizer&quot;&#125;等指定tokenizer文件</span></span><br></pre></td></tr></table></figure>
<p>解析<code>tokenizer_config.json</code>的<code>tokenizer_class</code>，如果不存在，则读取<code>config.json</code>中的数据</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config.json</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;architectures&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;BertForMaskedLM&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;attention_probs_dropout_prob&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="string">&quot;gradient_checkpointing&quot;</span>: false,</span><br><span class="line">  <span class="string">&quot;hidden_act&quot;</span>: <span class="string">&quot;gelu&quot;</span>,</span><br><span class="line">  <span class="string">&quot;hidden_dropout_prob&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="string">&quot;hidden_size&quot;</span>: <span class="number">768</span>,</span><br><span class="line">  <span class="string">&quot;initializer_range&quot;</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="string">&quot;intermediate_size&quot;</span>: <span class="number">3072</span>,</span><br><span class="line">  <span class="string">&quot;layer_norm_eps&quot;</span>: <span class="number">1e-12</span>,</span><br><span class="line">  <span class="string">&quot;max_position_embeddings&quot;</span>: <span class="number">512</span>,</span><br><span class="line">  <span class="string">&quot;model_type&quot;</span>: <span class="string">&quot;bert&quot;</span>,</span><br><span class="line">  <span class="string">&quot;num_attention_heads&quot;</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="string">&quot;num_hidden_layers&quot;</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="string">&quot;pad_token_id&quot;</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="string">&quot;position_embedding_type&quot;</span>: <span class="string">&quot;absolute&quot;</span>,</span><br><span class="line">  <span class="string">&quot;transformers_version&quot;</span>: <span class="string">&quot;4.6.0.dev0&quot;</span>,</span><br><span class="line">  <span class="string">&quot;type_vocab_size&quot;</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="string">&quot;use_cache&quot;</span>: true,</span><br><span class="line">  <span class="string">&quot;vocab_size&quot;</span>: <span class="number">30522</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可见，这里的<code>model_type</code>表明了模型类型为<code>bert</code>。<code>tokenizer_class</code>和<code>model_type</code>只要存在一个，都可以去内置的Tokenizer中匹配到对应的Tokenizer,然后使用<code>PreTrainedTokenizerBase(PreTrainedTokenizerFast).from_pretrained</code>去创建一个使用词表<code>vocab.txt</code>和分词器<code>tokenizer.json, tokenizer_config.json</code>作为参数构建的Tokenizer。(BertTokenizer等都是继承自PreTrainedTokenizerBase)</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">TOKENIZER_MAPPING_NAMES = OrderedDict(</span><br><span class="line">    [</span><br><span class="line">        (</span><br><span class="line">            <span class="string">&quot;albert&quot;</span>,</span><br><span class="line">            (</span><br><span class="line">                <span class="string">&quot;AlbertTokenizer&quot;</span> <span class="keyword">if</span> is_sentencepiece_available() <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                <span class="string">&quot;AlbertTokenizerFast&quot;</span> <span class="keyword">if</span> is_tokenizers_available() <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            ),</span><br><span class="line">        ),</span><br><span class="line">        (<span class="string">&quot;align&quot;</span>, (<span class="string">&quot;BertTokenizer&quot;</span>, <span class="string">&quot;BertTokenizerFast&quot;</span> <span class="keyword">if</span> is_tokenizers_available() <span class="keyword">else</span> <span class="literal">None</span>)),</span><br><span class="line">        (<span class="string">&quot;bark&quot;</span>, (<span class="string">&quot;BertTokenizer&quot;</span>, <span class="string">&quot;BertTokenizerFast&quot;</span> <span class="keyword">if</span> is_tokenizers_available() <span class="keyword">else</span> <span class="literal">None</span>)),</span><br><span class="line">        (<span class="string">&quot;bart&quot;</span>, (<span class="string">&quot;BartTokenizer&quot;</span>, <span class="string">&quot;BartTokenizerFast&quot;</span>)),</span><br><span class="line">        (</span><br><span class="line">            <span class="string">&quot;barthez&quot;</span>,</span><br><span class="line">            (</span><br><span class="line">                <span class="string">&quot;BarthezTokenizer&quot;</span> <span class="keyword">if</span> is_sentencepiece_available() <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                <span class="string">&quot;BarthezTokenizerFast&quot;</span> <span class="keyword">if</span> is_tokenizers_available() <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            ),</span><br><span class="line">        ),</span><br><span class="line">        (<span class="string">&quot;bartpho&quot;</span>, (<span class="string">&quot;BartphoTokenizer&quot;</span>, <span class="literal">None</span>)),</span><br><span class="line">        (<span class="string">&quot;bert&quot;</span>, (<span class="string">&quot;BertTokenizer&quot;</span>, <span class="string">&quot;BertTokenizerFast&quot;</span> <span class="keyword">if</span> is_tokenizers_available() <span class="keyword">else</span> <span class="literal">None</span>)),</span><br><span class="line">        (<span class="string">&quot;bert-generation&quot;</span>, (<span class="string">&quot;BertGenerationTokenizer&quot;</span> <span class="keyword">if</span> is_sentencepiece_available() <span class="keyword">else</span> <span class="literal">None</span>, <span class="literal">None</span>)),</span><br><span class="line">        (<span class="string">&quot;bert-japanese&quot;</span>, (<span class="string">&quot;BertJapaneseTokenizer&quot;</span>, <span class="literal">None</span>)),</span><br><span class="line">        (<span class="string">&quot;bertweet&quot;</span>, (<span class="string">&quot;BertweetTokenizer&quot;</span>, <span class="literal">None</span>)),</span><br><span class="line">        (</span><br><span class="line">            <span class="string">&quot;big_bird&quot;</span>,</span><br><span class="line">            (</span><br><span class="line">                <span class="string">&quot;BigBirdTokenizer&quot;</span> <span class="keyword">if</span> is_sentencepiece_available() <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                <span class="string">&quot;BigBirdTokenizerFast&quot;</span> <span class="keyword">if</span> is_tokenizers_available() <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            ),</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment">#...</span></span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>
<p>注意：<br>
tokenizer.json 文件是 Hugging Face 的 tokenizers 库用来存储预训练的tokenizer的配置和词汇表的文件。这个文件包含了词汇表（vocabulary）以及tokenizer的设置（例如特殊的token，如 [CLS], [SEP], [PAD]，以及词汇表的大小，是否使用lower case等等）。<br>
这个文件通常在你使用预训练的模型（如BERT, GPT-2等）进行微调（fine-tuning）时会用到，因为你需要用到和原始预训练模型相同的tokenizer来确保输入的处理方式一致。这个文件在你调用 from_pretrained 方法加载预训练模型时会自动加载。<br>
然而，如果你在构建自己的tokenizer时，你可能不会直接使用到这个文件。你可能会使用一些基础的tokenizer组件（如 BertTokenizer, GPT2Tokenizer 等）和你自己的词汇表来构建tokenizer。在这种情况下，你可能不会直接使用 tokenizer.json 文件，而是使用这些组件和你的词汇表来构建tokenizer。<br>
总的来说，tokenizer.json 文件是一个用来存储预训练tokenizer配置的文件，它在加载预训练模型时会用到，但在构建自定义tokenizer时可能不会直接使用。<br>
在Hugging Face的transformers库中，tokenizer.json文件主要是在使用fast版本的tokenizer时使用的。fast版本的tokenizer是用Rust编写的，性能更好，功能也更丰富。这些tokenizer可以通过tokenizers库单独使用，也可以通过transformers库使用。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;google-bert/bert-base-uncased&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;tokenizer type=&#x27;</span>,<span class="built_in">type</span>(tokenizer))</span><br><span class="line"><span class="comment"># tokenizer type= &lt;class &#x27;transformers.models.bert.tokenization_bert_fast.BertTokenizerFast&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="AutoModel到BertModel">AutoModel到BertModel</h3>
<p>那么，AutoModel是如何找到BertModel的?</p>
<ol>
<li>在AutoModel类中，会局部变量model_mapping初始化为<code>MODEL_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_MAPPING_NAMES)</code>来映射内部所有模型和配置和模型类的映射</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">CONFIG_MAPPING_NAMES = OrderedDict(</span><br><span class="line">    [</span><br><span class="line">        <span class="comment"># Add configs here</span></span><br><span class="line">        (<span class="string">&quot;albert&quot;</span>, <span class="string">&quot;AlbertConfig&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;align&quot;</span>, <span class="string">&quot;AlignConfig&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;altclip&quot;</span>, <span class="string">&quot;AltCLIPConfig&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;audio-spectrogram-transformer&quot;</span>, <span class="string">&quot;ASTConfig&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;autoformer&quot;</span>, <span class="string">&quot;AutoformerConfig&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;bark&quot;</span>, <span class="string">&quot;BarkConfig&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;bart&quot;</span>, <span class="string">&quot;BartConfig&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;beit&quot;</span>, <span class="string">&quot;BeitConfig&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;bert&quot;</span>, <span class="string">&quot;BertConfig&quot;</span>),</span><br><span class="line">        <span class="comment">#......</span></span><br><span class="line">    ])</span><br><span class="line">MODEL_MAPPING_NAMES = OrderedDict(</span><br><span class="line">    [</span><br><span class="line">        <span class="comment"># Base model mapping</span></span><br><span class="line">        (<span class="string">&quot;albert&quot;</span>, <span class="string">&quot;AlbertModel&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;align&quot;</span>, <span class="string">&quot;AlignModel&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;altclip&quot;</span>, <span class="string">&quot;AltCLIPModel&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;audio-spectrogram-transformer&quot;</span>, <span class="string">&quot;ASTModel&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;autoformer&quot;</span>, <span class="string">&quot;AutoformerModel&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;bark&quot;</span>, <span class="string">&quot;BarkModel&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;bart&quot;</span>, <span class="string">&quot;BartModel&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;beit&quot;</span>, <span class="string">&quot;BeitModel&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;bert&quot;</span>, <span class="string">&quot;BertModel&quot;</span>),</span><br><span class="line">        <span class="comment">#......</span></span><br><span class="line">    ])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>首先执行<code>AutoModel.from_pretrained</code>函数，先拉取google-bert/bert-base-uncased中的<code>config.json</code>文件，并使用<code>AutoConfig.from_pretrained</code>构建一个类型为<code>=&lt;class 'transformers.models.bert.configuration_bert.BertConfig'&gt;</code>的对象。原理为从config.json中的model_type为bert来从CONFIG_MAPPING_NAMES加载对应的Config类</li>
<li>根据构建的BertConfig结构，在model_mapping中找到映射到的具体Model类，即BertModel</li>
</ol>
<h2 id="pytorch导学：bert分类实战">pytorch导学：bert分类实战</h2>
<h3 id="数据集">数据集</h3>
<p>数据来源：今日头条客户端 ；数据规模：共382688条，分布于15个分类中。每行为一条数据，以_!_分割的个字段，从前往后分别是 新闻ID，分类code（见下文），分类名称（见下文），新闻字符串（仅含标题），新闻关键词。原始数据集格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">tail</span> -n 10 toutiao_cat_data.txt</span><br><span class="line"><span class="comment"># 6554596645437178371_!_104_!_news_finance_!_百万亏损中悟出的交易之道_!_齐威王,田忌赛马,忌讳,同龄人,交易者,好朋友,做交易,股民,股票市场,田忌</span></span><br><span class="line"><span class="comment"># 6554627652047602190_!_107_!_news_car_!_这款合资车曾比朗逸还火 现在却成功“跳楼” 售价仅为8万！_!_科鲁兹,小轿车,雪佛兰,大众朗逸,SUV</span></span><br><span class="line"><span class="comment"># 6554357384574140676_!_107_!_news_car_!_汽车防撞梁对安全的意义大吗？_!_</span></span><br><span class="line"><span class="comment"># 6554661690015744516_!_107_!_news_car_!_精致实用，这辆房车专为行家准备_!_车内,C200,房车,水曲柳,依维柯,玻璃钢</span></span><br><span class="line"><span class="comment"># 6554468366101250311_!_109_!_news_tech_!_如果联想给华为的短码投票，中国的5G是否拥有专利权？是否还能挽回？_!_</span></span><br><span class="line"><span class="comment"># 6554578634403741966_!_109_!_news_tech_!_A10处理器iPhone SE二代值得期待吗？_!_</span></span><br><span class="line"><span class="comment"># 6554623450374209806_!_110_!_news_military_!_先进战机叛逃将带来重大损失，美军如何防止F22飞行员驾机叛逃？_!_</span></span><br><span class="line"><span class="comment"># 6554489948580348424_!_113_!_news_world_!_又一国领导人放话，只要普京下令，数万大军“碾压”美国白宫！_!_以色列,普京,俄罗斯,叙利亚,车臣</span></span><br><span class="line"><span class="comment"># 6554706019040100611_!_113_!_news_world_!_如何看待美国总统连续撕毁美国签署的国际协议的举动？_!_</span></span><br><span class="line"><span class="comment"># 6554360505438306824_!_115_!_news_agriculture_!_农博会上，公安100余种土特产成了抢手货……_!_特色农产品,农博会,荆州市,生物科技,公安县</span></span><br></pre></td></tr></table></figure>
<p>分类code与名称：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">100 民生 故事 news_story</span><br><span class="line">101 文化 文化 news_culture</span><br><span class="line">102 娱乐 娱乐 news_entertainment</span><br><span class="line">103 体育 体育 news_sports</span><br><span class="line">104 财经 财经 news_finance</span><br><span class="line">106 房产 房产 news_house</span><br><span class="line">107 汽车 汽车 news_car</span><br><span class="line">108 教育 教育 news_edu </span><br><span class="line">109 科技 科技 news_tech</span><br><span class="line">110 军事 军事 news_military</span><br><span class="line">112 旅游 旅游 news_travel</span><br><span class="line">113 国际 国际 news_world</span><br><span class="line">114 证券 股票 stock</span><br><span class="line">115 农业 三农 news_agriculture</span><br><span class="line">116 电竞 游戏 news_game</span><br></pre></td></tr></table></figure>
<p>采集时间： 2018年05月<br>
数据集分割：以0.7 0.15 0.15做分割</p>
<h3 id="数据集处理">数据集处理</h3>
<p>封装<code>dataset.py</code>文件，包含数据集的处理</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Tuple</span>,<span class="type">Any</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(data.Dataset):</span><br><span class="line">    classes = [</span><br><span class="line">        [<span class="number">100</span>, <span class="string">&#x27;民生 故事&#x27;</span>, <span class="string">&#x27;news_story&#x27;</span>],</span><br><span class="line">        [<span class="number">101</span>, <span class="string">&#x27;文化 文化&#x27;</span>, <span class="string">&#x27;news_culture&#x27;</span>],</span><br><span class="line">        [<span class="number">102</span>, <span class="string">&#x27;娱乐 娱乐&#x27;</span>, <span class="string">&#x27;news_entertainment&#x27;</span>],</span><br><span class="line">        [<span class="number">103</span>, <span class="string">&#x27;体育 体育&#x27;</span>, <span class="string">&#x27;news_sports&#x27;</span>],</span><br><span class="line">        [<span class="number">104</span>, <span class="string">&#x27;财经 财经&#x27;</span>, <span class="string">&#x27;news_finance&#x27;</span>],</span><br><span class="line">        <span class="comment"># [105, &#x27;时政 新时代&#x27;, &#x27;nineteenth&#x27;],</span></span><br><span class="line">        [<span class="number">106</span>, <span class="string">&#x27;房产 房产&#x27;</span>, <span class="string">&#x27;news_house&#x27;</span>],</span><br><span class="line">        [<span class="number">107</span>, <span class="string">&#x27;汽车 汽车&#x27;</span>, <span class="string">&#x27;news_car&#x27;</span>],</span><br><span class="line">        [<span class="number">108</span>, <span class="string">&#x27;教育 教育&#x27;</span>, <span class="string">&#x27;news_edu&#x27;</span> ],</span><br><span class="line">        [<span class="number">109</span>, <span class="string">&#x27;科技 科技&#x27;</span>, <span class="string">&#x27;news_tech&#x27;</span>],</span><br><span class="line">        [<span class="number">110</span>, <span class="string">&#x27;军事 军事&#x27;</span>, <span class="string">&#x27;news_military&#x27;</span>],</span><br><span class="line">        <span class="comment"># [111 宗教 无，凤凰佛教等来源],</span></span><br><span class="line">        [<span class="number">112</span>, <span class="string">&#x27;旅游 旅游&#x27;</span>, <span class="string">&#x27;news_travel&#x27;</span>],</span><br><span class="line">        [<span class="number">113</span>, <span class="string">&#x27;国际 国际&#x27;</span>, <span class="string">&#x27;news_world&#x27;</span>],</span><br><span class="line">        [<span class="number">114</span>, <span class="string">&#x27;证券 股票&#x27;</span>, <span class="string">&#x27;stock&#x27;</span>],</span><br><span class="line">        [<span class="number">115</span>, <span class="string">&#x27;农业 三农&#x27;</span>, <span class="string">&#x27;news_agriculture&#x27;</span>],</span><br><span class="line">        [<span class="number">116</span>, <span class="string">&#x27;电竞 游戏&#x27;</span>, <span class="string">&#x27;news_game&#x27;</span>]</span><br><span class="line">    ]</span><br><span class="line">    classes_map=&#123;</span><br><span class="line">        <span class="string">&quot;100&quot;</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;101&quot;</span>:<span class="number">1</span>,</span><br><span class="line">        <span class="string">&quot;102&quot;</span>:<span class="number">2</span>,</span><br><span class="line">        <span class="string">&quot;103&quot;</span>:<span class="number">3</span>,</span><br><span class="line">        <span class="string">&quot;104&quot;</span>:<span class="number">4</span>,</span><br><span class="line">        <span class="string">&quot;106&quot;</span>:<span class="number">5</span>,</span><br><span class="line">        <span class="string">&quot;107&quot;</span>:<span class="number">6</span>,</span><br><span class="line">        <span class="string">&quot;108&quot;</span>:<span class="number">7</span>,</span><br><span class="line">        <span class="string">&quot;109&quot;</span>:<span class="number">8</span>,</span><br><span class="line">        <span class="string">&quot;110&quot;</span>:<span class="number">9</span>,</span><br><span class="line">        <span class="string">&quot;112&quot;</span>:<span class="number">10</span>,</span><br><span class="line">        <span class="string">&quot;113&quot;</span>:<span class="number">11</span>,</span><br><span class="line">        <span class="string">&quot;114&quot;</span>:<span class="number">12</span>,</span><br><span class="line">        <span class="string">&quot;115&quot;</span>:<span class="number">13</span>,</span><br><span class="line">        <span class="string">&quot;116&quot;</span>:<span class="number">14</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            root:<span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">            train:<span class="built_in">bool</span> = <span class="literal">True</span></span></span><br><span class="line"><span class="params">            </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;训练集采用前80%，测试集采用后10%&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.root = root</span><br><span class="line">        self.train = train</span><br><span class="line">        </span><br><span class="line">        self.df : pd.DataFrame = pd.DataFrame(columns=[<span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;text&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        self._load_data()</span><br><span class="line">        self.df = self.df.head(<span class="number">5000</span>)  <span class="comment">#时间原因，我只取了1600条训练</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;is_train_data=<span class="subst">&#123;self.train&#125;</span>, total_len=<span class="subst">&#123;<span class="built_in">len</span>(self.df)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="comment"># print(f&quot;head data=&#123;self.df.head(10)&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_load_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;从&#123;root&#125;/toutiao_cat_data.txt或者&#123;root&#125;/toutiao_cat_data.csv中加载数据到pandas DataFrame中&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(self.root, <span class="string">&quot;toutiao_cat_data.csv&quot;</span>)):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;从txt中加载数据&quot;&quot;&quot;</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(self.root,<span class="string">&quot;toutiao_cat_data.txt&quot;</span>),<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">                count = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">                    count = count +<span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span>(count % <span class="number">100</span> == <span class="number">0</span>):</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">f&#x27;line:<span class="subst">&#123;count&#125;</span>, items=<span class="subst">&#123;items&#125;</span>&#x27;</span>)</span><br><span class="line">                    <span class="comment"># 6552431613437805063_!_102_!_news_entertainment_!_谢娜为李浩菲澄清网络谣言，之后她的两个行为给自己加分_!_佟丽娅,网络谣言,快乐大本营,李浩菲,谢娜,观众们</span></span><br><span class="line">                    items = line.split(<span class="string">&#x27;_!_&#x27;</span>)  </span><br><span class="line">                    label, text = items[<span class="number">1</span>], items[<span class="number">3</span>]+<span class="string">&quot;,&quot;</span>+items[<span class="number">4</span>] </span><br><span class="line">                    self.df = self.df._append(&#123;<span class="string">&#x27;label&#x27;</span>: label, <span class="string">&#x27;text&#x27;</span>: text&#125;, ignore_index=<span class="literal">True</span>)</span><br><span class="line">                self.df.to_csv(os.path.join(self.root,<span class="string">&#x27;toutiao_cat_data.csv&#x27;</span>), index=<span class="literal">False</span>, header=<span class="literal">True</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;txt total count=<span class="subst">&#123;count&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="comment"># 计算各部分数据集的大小</span></span><br><span class="line">                total_len = <span class="built_in">len</span>(self.df)</span><br><span class="line">                train_len = <span class="built_in">int</span>(total_len * <span class="number">0.8</span>)</span><br><span class="line">                valid_len = <span class="built_in">int</span>(total_len * <span class="number">0.1</span>)</span><br><span class="line">                <span class="comment"># 分割数据集</span></span><br><span class="line">                df_train = self.df.iloc[:train_len]</span><br><span class="line">                df_valid = self.df.iloc[train_len:train_len+valid_len]</span><br><span class="line">                df_test = self.df.iloc[train_len+valid_len:]</span><br><span class="line">                df_train.to_csv(os.path.join(self.root,<span class="string">&#x27;toutiao_cat_data_train.csv&#x27;</span>), index=<span class="literal">False</span>, header=<span class="literal">True</span>)</span><br><span class="line">                df_valid.to_csv(os.path.join(self.root,<span class="string">&#x27;toutiao_cat_data_valid.csv&#x27;</span>), index=<span class="literal">False</span>, header=<span class="literal">True</span>)</span><br><span class="line">                df_test.to_csv(os.path.join(self.root,<span class="string">&#x27;toutiao_cat_data_test.csv&#x27;</span>), index=<span class="literal">False</span>, header=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据是否是训练集来加载数据到pd中</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            self.df = pd.read_csv(os.path.join(self.root,<span class="string">&#x27;toutiao_cat_data_train.csv&#x27;</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.df = pd.read_csv(os.path.join(self.root,<span class="string">&#x27;toutiao_cat_data_valid.csv&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.df)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>,<span class="type">Any</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;继承Dataset接口，根据index获取某个元素，返回数据和标签&quot;&quot;&quot;</span></span><br><span class="line">        label = self.df.iloc[index][<span class="string">&quot;label&quot;</span>]</span><br><span class="line">        text = self.df.iloc[index][<span class="string">&quot;text&quot;</span>]</span><br><span class="line">        <span class="comment"># print(f&quot;__getitem__, type(label)=&#123;type(label)&#125;,label=&#123;label&#125;&quot;)</span></span><br><span class="line">        <span class="keyword">return</span> self.classes_map[<span class="built_in">str</span>(label)],text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    data = Dataset(<span class="string">&quot;toutiao-text-classfication-dataset&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;len=<span class="subst">&#123;<span class="built_in">len</span>(data)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="自定义封装bert模型训练推理">自定义封装bert模型训练推理</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel ,BertForSequenceClassification</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span></span><br><span class="line"><span class="keyword">import</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BertClassification</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                dropout=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                num_labels = <span class="number">15</span>,</span></span><br><span class="line"><span class="params">                </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 修改了类别数量不会影响加载预训练参数</span></span><br><span class="line">        self.bert = BertModel.from_pretrained(<span class="string">&quot;bert-base-chinese&quot;</span>,num_labels=num_labels)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.classifier = nn.Linear(<span class="number">768</span>, num_labels)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,input_ids, mask</span>):</span><br><span class="line">        _, pooled_output = self.bert(input_ids= input_ids, attention_mask=mask, return_dict=<span class="literal">False</span>)</span><br><span class="line">        dropout_output = self.dropout(pooled_output)</span><br><span class="line">        classifier_output = self.classifier(dropout_output)</span><br><span class="line">        final_layer = self.relu(classifier_output)</span><br><span class="line">        <span class="keyword">return</span> final_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params"></span></span><br><span class="line"><span class="params">        dataloader:DataLoader,</span></span><br><span class="line"><span class="params">        model:nn.Module,</span></span><br><span class="line"><span class="params">        learning_rate:<span class="built_in">float</span>=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">        epochs:<span class="built_in">int</span>=<span class="number">16</span></span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    model.train()</span><br><span class="line">    device = ( <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> )</span><br><span class="line">    <span class="keyword">if</span> device == <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">        model = model.cuda()</span><br><span class="line">    <span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">    loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">    tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    <span class="comment"># 开始进入训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch_num <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch_num+<span class="number">1</span>&#125;</span>\n-------------------------------&quot;</span>)</span><br><span class="line">        <span class="comment"># 定义两个变量，用于存储训练集的准确率和损失</span></span><br><span class="line">        total_acc_train = <span class="number">0</span></span><br><span class="line">        total_loss_train = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch,(labels, texts) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            train_inputs = tokenizer(texts, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">            train_labels = labels.to(device)</span><br><span class="line">            input_id = train_inputs[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">1</span>).to(device)</span><br><span class="line">            mask = train_inputs[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">            <span class="comment"># print(f&quot;input_id:&#123;input_id.size()&#125;, mask:&#123;mask.size()&#125;&quot;)</span></span><br><span class="line">            output = model(input_id, mask)</span><br><span class="line">            batch_loss = loss_fn(output, train_labels.long())</span><br><span class="line">            <span class="comment"># item从tensor提取单个元素</span></span><br><span class="line">            total_loss_train += batch_loss.item()</span><br><span class="line">            <span class="comment"># 计算精度</span></span><br><span class="line">            acc = (output.argmax(dim=<span class="number">1</span>) == train_labels).<span class="built_in">sum</span>().item()</span><br><span class="line">            total_acc_train += acc</span><br><span class="line">            <span class="comment"># print(f&quot;loss=&#123;batch_loss&#125;,acc=&#123;acc&#125;&quot;)</span></span><br><span class="line">            <span class="comment"># 模型更新，反向传播</span></span><br><span class="line">            model.zero_grad()</span><br><span class="line">            batch_loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">if</span> batch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                loss, current = batch_loss.item(), (batch + <span class="number">1</span>) * <span class="built_in">len</span>(labels)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;loss: <span class="subst">&#123;loss:&gt;7f&#125;</span>  [<span class="subst">&#123;current:&gt;5d&#125;</span>/<span class="subst">&#123;size:&gt;5d&#125;</span>]&quot;</span>)</span><br><span class="line">        total_loss_train /= <span class="built_in">len</span>(dataloader)</span><br><span class="line">        total_acc_train /= size</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Train epoch[<span class="subst">&#123;epoch_num+<span class="number">1</span>&#125;</span>]: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*total_acc_train):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;total_loss_train:&gt;8f&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params"></span></span><br><span class="line"><span class="params">        dataloader:DataLoader,</span></span><br><span class="line"><span class="params">        model:nn.Module,    </span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    batch_nums = <span class="built_in">len</span>(dataloader)</span><br><span class="line">    tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">    device = ( <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> )</span><br><span class="line">    <span class="keyword">if</span> device == <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">        model = model.cuda()</span><br><span class="line">    test_loss, test_acc = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> labels, texts <span class="keyword">in</span> dataloader:</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line">            inputs = tokenizer(texts, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">            mask = inputs[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">            input_id = inputs[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">1</span>).to(device)</span><br><span class="line">            output = model(input_id,mask)</span><br><span class="line">            test_loss += loss_fn(output,labels.long()).item()</span><br><span class="line">            test_acc += (output.argmax(dim=<span class="number">1</span>) == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    test_loss /= batch_nums</span><br><span class="line">    test_acc /= size</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Test result: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*test_acc):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_model</span>(<span class="params"></span></span><br><span class="line"><span class="params">        text:<span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">        model:BertClassification</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">    device = ( <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> )</span><br><span class="line">   </span><br><span class="line">    tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">if</span> device == <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">        model = model.cuda()</span><br><span class="line">    text_input  = tokenizer(text, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    mask = text_input[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">    input_id = text_input[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">    output = model(input_id, mask)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;output=<span class="subst">&#123;output&#125;</span>&#x27;</span>)</span><br><span class="line">    output = output.argmax(dim=<span class="number">1</span>)</span><br><span class="line">    output = output.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;output label id=<span class="subst">&#123;output&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size = <span class="number">32</span></span><br><span class="line">    lr = <span class="number">1e-6</span></span><br><span class="line">    epoch = <span class="number">10</span></span><br><span class="line">    model = BertClassification()</span><br><span class="line">    <span class="comment"># 模型训练并保存模型</span></span><br><span class="line">    train_data = dataset.Dataset(<span class="string">&quot;toutiao-text-classfication-dataset&quot;</span>)</span><br><span class="line">    train_loader = DataLoader(train_data,batch_size=batch_size)</span><br><span class="line">    train(train_loader, model,lr,epochs=epoch)</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&quot;bertclassify_model.pth&quot;</span>)</span><br><span class="line">    <span class="comment"># 加载模型进行测试</span></span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&quot;bertclassify_model.pth&quot;</span>))</span><br><span class="line">    test_data = dataset.Dataset(<span class="string">&quot;toutiao-text-classfication-dataset&quot;</span>,train=<span class="literal">False</span>)</span><br><span class="line">    test_loader = DataLoader(test_data,batch_size=batch_size)</span><br><span class="line">    test(test_loader,model)</span><br><span class="line">    <span class="comment"># 模型推理</span></span><br><span class="line">    test_model(<span class="string">&quot;京城最值得你来场文化之旅的博物馆保利集团,马未都,中国科学技术馆,博物馆,新中国&quot;</span>,model)</span><br></pre></td></tr></table></figure>
<h3 id="使用BertForSequenceClassification训练推理">使用BertForSequenceClassification训练推理</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel ,BertForSequenceClassification</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer,BertConfig</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span></span><br><span class="line"><span class="keyword">import</span> dataset</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params"></span></span><br><span class="line"><span class="params">        dataloader:DataLoader,</span></span><br><span class="line"><span class="params">        model:nn.Module,</span></span><br><span class="line"><span class="params">        learning_rate:<span class="built_in">float</span>=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">        epochs:<span class="built_in">int</span>=<span class="number">16</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">    device = ( <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> )</span><br><span class="line">    <span class="keyword">if</span> device == <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">        model = model.cuda()</span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    batch_nums = <span class="built_in">len</span>(dataloader)</span><br><span class="line">    <span class="comment"># 定义优化器</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">    <span class="comment"># optimizer = torch.optim.AdamW(model.model.parameters(), lr=learning_rate)</span></span><br><span class="line">    tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch_num <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch_num+<span class="number">1</span>&#125;</span>\n-------------------------------&quot;</span>)</span><br><span class="line">        total_acc_train, total_loss_train = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch,(labels, texts) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            train_inputs = tokenizer(texts, padding=<span class="literal">True</span>, </span><br><span class="line">                                     truncation=<span class="literal">True</span>, </span><br><span class="line">                                     return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">            train_labels = labels.to(device)</span><br><span class="line">            input_id = train_inputs[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">1</span>).to(device)</span><br><span class="line">            mask = train_inputs[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">            outputs = model(input_id, attention_mask=mask, labels=train_labels)</span><br><span class="line">            <span class="comment"># print(f&quot;outputs=&#123;outputs.logits&#125;, labels=&#123;train_labels.size()&#125;&quot;)</span></span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss = outputs.loss</span><br><span class="line">            <span class="comment"># 计算精度</span></span><br><span class="line">            acc = (outputs.logits.argmax(dim=<span class="number">1</span>) == train_labels).<span class="built_in">sum</span>().item()</span><br><span class="line">            total_loss_train += loss</span><br><span class="line">            total_acc_train += acc</span><br><span class="line">            <span class="comment"># print(f&quot;loss=&#123;loss&#125;&quot;)</span></span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># 更新权重</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="comment"># 清空梯度</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">if</span> batch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                loss, current = loss.item(), (batch + <span class="number">1</span>) * <span class="built_in">len</span>(labels)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;loss: <span class="subst">&#123;loss:&gt;7f&#125;</span>  [<span class="subst">&#123;current:&gt;5d&#125;</span>/<span class="subst">&#123;size:&gt;5d&#125;</span>]&quot;</span>)</span><br><span class="line"></span><br><span class="line">        total_loss_train /= batch_nums</span><br><span class="line">        total_acc_train /= size</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Train epoch[<span class="subst">&#123;epoch_num+<span class="number">1</span>&#125;</span>]: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*total_acc_train):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;total_loss_train:&gt;8f&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params"></span></span><br><span class="line"><span class="params">        dataloader:DataLoader,</span></span><br><span class="line"><span class="params">        model:nn.Module,    </span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    batch_nums = <span class="built_in">len</span>(dataloader)</span><br><span class="line">    tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    device = ( <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> )</span><br><span class="line">    <span class="keyword">if</span> device == <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">        model = model.cuda()</span><br><span class="line">    test_loss, test_acc = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> labels, texts <span class="keyword">in</span> dataloader:</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line">            inputs = tokenizer(texts, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">            mask = inputs[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">            input_id = inputs[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">1</span>).to(device)</span><br><span class="line">            output = model(input_id,mask, labels=labels)</span><br><span class="line">            test_acc += (output.logits.argmax(dim=<span class="number">1</span>) == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    test_loss /= batch_nums</span><br><span class="line">    test_acc /= size</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Test result: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*test_acc):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_model</span>(<span class="params"></span></span><br><span class="line"><span class="params">        text:<span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">        model:nn.Module</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">    device = ( <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> )</span><br><span class="line">   </span><br><span class="line">    tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">if</span> device == <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">        model = model.cuda()</span><br><span class="line">    text_input  = tokenizer(text, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    mask = text_input[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">    input_id = text_input[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">    output = model(input_id, mask)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;output=<span class="subst">&#123;output&#125;</span>&#x27;</span>)</span><br><span class="line">    output = output.logits.argmax(dim=<span class="number">1</span>)</span><br><span class="line">    output = output.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;output label id=<span class="subst">&#123;output&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size = <span class="number">32</span></span><br><span class="line">    lr = <span class="number">1e-6</span></span><br><span class="line">    epochs = <span class="number">10</span></span><br><span class="line">    config = BertConfig.from_pretrained(<span class="string">&quot;bert-base-chinese&quot;</span>,</span><br><span class="line">                                        num_labels=<span class="number">15</span>, </span><br><span class="line">                                        output_attentions=<span class="literal">False</span>, <span class="comment"># 模型是否返回 attentions weights.</span></span><br><span class="line">                                        output_hidden_states=<span class="literal">False</span>, <span class="comment"># 模型是否返回所有隐藏状态</span></span><br><span class="line">                                        )</span><br><span class="line">    model = BertForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-chinese&quot;</span>,config=config)</span><br><span class="line">    <span class="comment"># 模型训练并保存模型</span></span><br><span class="line">    train_data = dataset.Dataset(<span class="string">&quot;toutiao-text-classfication-dataset&quot;</span>)</span><br><span class="line">    train_loader = DataLoader(train_data,batch_size=batch_size)</span><br><span class="line">    train(train_loader, model,lr,epochs=epochs)</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&quot;bertsequenceclassify_model.pth&quot;</span>)</span><br><span class="line">    <span class="comment"># 加载模型进行测试</span></span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&quot;bertsequenceclassify_model.pth&quot;</span>))</span><br><span class="line">    test_data = dataset.Dataset(<span class="string">&quot;toutiao-text-classfication-dataset&quot;</span>,train=<span class="literal">False</span>)</span><br><span class="line">    test_loader = DataLoader(test_data,batch_size=batch_size)</span><br><span class="line">    test(test_loader,model)</span><br><span class="line">    <span class="comment"># 模型推理</span></span><br><span class="line">    test_model(<span class="string">&quot;京城最值得你来场文化之旅的博物馆保利集团,马未都,中国科学技术馆,博物馆,新中国&quot;</span>,model)</span><br></pre></td></tr></table></figure>
<h2 id="参考">参考</h2>
<p>头条评论分类：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41301570/article/details/134320018">https://blog.csdn.net/qq_41301570/article/details/134320018</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://gitcode.com/skdjfla/toutiao-text-classfication-dataset.git</span><br></pre></td></tr></table></figure>
<p>斯坦福情感分类:<a target="_blank" rel="noopener" href="https://blog.csdn.net/a553181867/article/details/105389757">https://blog.csdn.net/a553181867/article/details/105389757</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://mylofty.github.io">JimmyDing</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://mylofty.github.io/2024/05/11/3148f0b756e9/">https://mylofty.github.io/2024/05/11/3148f0b756e9/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://mylofty.github.io" target="_blank">爱开源GoGo</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ai/">ai</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/07/225dc84728ab/" title="大模型意图选择"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">大模型意图选择</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/10/981ace98feb5/" title="pytorch基础：FashionMNIST时装分类"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">pytorch基础：FashionMNIST时装分类</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/06/11/fa22652bbbc5/" title="RAG应用"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-11</div><div class="title">RAG应用</div></div></a></div><div><a href="/2024/03/31/df8ae9e77565/" title="comfyui使用"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-31</div><div class="title">comfyui使用</div></div></a></div><div><a href="/2024/04/20/2f43d59ec67f/" title="huggingface镜像站使用"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-20</div><div class="title">huggingface镜像站使用</div></div></a></div><div><a href="/2024/05/10/981ace98feb5/" title="pytorch基础：FashionMNIST时装分类"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-10</div><div class="title">pytorch基础：FashionMNIST时装分类</div></div></a></div><div><a href="/2024/03/31/e135a5da70e3/" title="transformer详解"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-31</div><div class="title">transformer详解</div></div></a></div><div><a href="/2024/06/07/225dc84728ab/" title="大模型意图选择"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-07</div><div class="title">大模型意图选择</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/notebook.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">JimmyDing</div><div class="author-info__description">种一棵树，最好的时间是十年前，其次是现在</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">32</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mylofty"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎前来爱开源GoGo</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#bert%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8"><span class="toc-text">bert模型使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#huggingface%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-text">huggingface中的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BDbert%E6%A8%A1%E5%9E%8B"><span class="toc-text">加载bert模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert%E4%B8%AD%E5%88%86%E8%AF%8Dtokenize"><span class="toc-text">bert中分词tokenize</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AutoTokenizer%E5%88%B0bert-tokenizer"><span class="toc-text">AutoTokenizer到bert tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AutoModel%E5%88%B0BertModel"><span class="toc-text">AutoModel到BertModel</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E5%AF%BC%E5%AD%A6%EF%BC%9Abert%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98"><span class="toc-text">pytorch导学：bert分类实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86"><span class="toc-text">数据集处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B0%81%E8%A3%85bert%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86"><span class="toc-text">自定义封装bert模型训练推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8BertForSequenceClassification%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86"><span class="toc-text">使用BertForSequenceClassification训练推理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-text">参考</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/05/09/045d8c332459/" title="vllm使用教程-基于qwen模型"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vllm使用教程-基于qwen模型"/></a><div class="content"><a class="title" href="/2025/05/09/045d8c332459/" title="vllm使用教程-基于qwen模型">vllm使用教程-基于qwen模型</a><time datetime="2025-05-09T06:51:21.000Z" title="发表于 2025-05-09 14:51:21">2025-05-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/14/36b4391cdd6f/" title="解锁AI潜能：万字详解大语言模型提示工程的终极指南"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="解锁AI潜能：万字详解大语言模型提示工程的终极指南"/></a><div class="content"><a class="title" href="/2024/08/14/36b4391cdd6f/" title="解锁AI潜能：万字详解大语言模型提示工程的终极指南">解锁AI潜能：万字详解大语言模型提示工程的终极指南</a><time datetime="2024-08-14T06:51:21.000Z" title="发表于 2024-08-14 14:51:21">2024-08-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/11/fa22652bbbc5/" title="RAG应用"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="RAG应用"/></a><div class="content"><a class="title" href="/2024/06/11/fa22652bbbc5/" title="RAG应用">RAG应用</a><time datetime="2024-06-11T06:51:21.000Z" title="发表于 2024-06-11 14:51:21">2024-06-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/07/225dc84728ab/" title="大模型意图选择"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大模型意图选择"/></a><div class="content"><a class="title" href="/2024/06/07/225dc84728ab/" title="大模型意图选择">大模型意图选择</a><time datetime="2024-06-07T06:51:21.000Z" title="发表于 2024-06-07 14:51:21">2024-06-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/11/3148f0b756e9/" title="pytorch导学：使用bert实现分类任务"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="pytorch导学：使用bert实现分类任务"/></a><div class="content"><a class="title" href="/2024/05/11/3148f0b756e9/" title="pytorch导学：使用bert实现分类任务">pytorch导学：使用bert实现分类任务</a><time datetime="2024-05-11T06:51:21.000Z" title="发表于 2024-05-11 14:51:21">2024-05-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/10/981ace98feb5/" title="pytorch基础：FashionMNIST时装分类"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="pytorch基础：FashionMNIST时装分类"/></a><div class="content"><a class="title" href="/2024/05/10/981ace98feb5/" title="pytorch基础：FashionMNIST时装分类">pytorch基础：FashionMNIST时装分类</a><time datetime="2024-05-10T06:51:21.000Z" title="发表于 2024-05-10 14:51:21">2024-05-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/04/26/111ea67534ab/" title="redis基础知识"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="redis基础知识"/></a><div class="content"><a class="title" href="/2024/04/26/111ea67534ab/" title="redis基础知识">redis基础知识</a><time datetime="2024-04-25T16:00:00.000Z" title="发表于 2024-04-26 00:00:00">2024-04-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/04/23/a07a83071c8a/" title="腹肌锻炼"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="腹肌锻炼"/></a><div class="content"><a class="title" href="/2024/04/23/a07a83071c8a/" title="腹肌锻炼">腹肌锻炼</a><time datetime="2024-04-23T06:51:21.000Z" title="发表于 2024-04-23 14:51:21">2024-04-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By JimmyDing</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>